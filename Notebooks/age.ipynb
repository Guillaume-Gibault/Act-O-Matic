{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-22T16:37:57.884438Z",
     "start_time": "2025-01-22T16:36:51.599355Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Charger les données depuis un fichier CSV\n",
    "data = pd.read_csv(r\"C:\\Users\\33658\\PycharmProjects\\Act-O-Matic\\Datasets\\FaceCropAge\\Brad Pitt Crop\\index_cleaned.csv\", delimiter=\";\")\n",
    "\n",
    "# Charger et prétraiter les images\n",
    "def preprocess_data(data):\n",
    "    images, ages = [], []\n",
    "    base_path = r\"C:\\Users\\33658\\PycharmProjects\\Act-O-Matic\\Datasets\\FaceCropAge\\Brad Pitt Crop\"\n",
    "    for i, row in data.iterrows():\n",
    "        img_name = str(row[\"Image\"]) + \"_face\"+ \".jpg\" \n",
    "        img_path = os.path.join(base_path, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"Image non trouvée : {img_path}\")\n",
    "            continue\n",
    "        img = cv2.resize(img, (224, 224))   # Redimensionner\n",
    "        img = img / 255.0                   # Normaliser\n",
    "        images.append(img)\n",
    "        ages.append(float(row[\"Age\"]))\n",
    "    return np.array(images), np.array(ages)\n",
    "\n",
    "# Prétraiter les données\n",
    "images, ages = preprocess_data(data)\n",
    "\n",
    "# Diviser en ensembles d'entraînement et de validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(images, ages, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convertir en tenseurs\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32).shuffle(1000)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)\n",
    "\n",
    "# Charger le modèle pré-entraîné MobileNetV2\n",
    "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Ajouter des couches personnalisées\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(1, activation='linear')(x)  # Activation linéaire pour une régression\n",
    "\n",
    "# Construire le modèle\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Geler les couches du modèle pré-entraîné\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=\"mean_absolute_error\",  # Erreur absolue moyenne pour la régression\n",
    "    metrics=[\"mae\"]  # Suivi de la MAE\n",
    ")\n",
    "\n",
    "# Ajouter un callback pour l'early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=100,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "model.save(\"age_estimation_model_brad.h5\")\n",
    "print(\"Modèle sauvegardé sous 'age_estimation_model_brad.h5'\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 352ms/step - loss: 37.3635 - mae: 37.3635 - val_loss: 14.5103 - val_mae: 14.5103\n",
      "Epoch 2/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 234ms/step - loss: 13.1928 - mae: 13.1928 - val_loss: 12.4047 - val_mae: 12.4047\n",
      "Epoch 3/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 228ms/step - loss: 12.0529 - mae: 12.0529 - val_loss: 8.8535 - val_mae: 8.8535\n",
      "Epoch 4/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 227ms/step - loss: 9.3971 - mae: 9.3971 - val_loss: 9.5534 - val_mae: 9.5534\n",
      "Epoch 5/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 238ms/step - loss: 10.1483 - mae: 10.1483 - val_loss: 7.8707 - val_mae: 7.8707\n",
      "Epoch 6/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 236ms/step - loss: 8.2782 - mae: 8.2782 - val_loss: 7.6523 - val_mae: 7.6523\n",
      "Epoch 7/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 237ms/step - loss: 8.1506 - mae: 8.1506 - val_loss: 7.5950 - val_mae: 7.5950\n",
      "Epoch 8/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 242ms/step - loss: 8.1404 - mae: 8.1404 - val_loss: 7.2580 - val_mae: 7.2580\n",
      "Epoch 9/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 240ms/step - loss: 8.3274 - mae: 8.3274 - val_loss: 7.1545 - val_mae: 7.1545\n",
      "Epoch 10/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 265ms/step - loss: 8.0429 - mae: 8.0429 - val_loss: 7.1176 - val_mae: 7.1176\n",
      "Epoch 11/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 289ms/step - loss: 6.9548 - mae: 6.9548 - val_loss: 6.9590 - val_mae: 6.9590\n",
      "Epoch 12/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 287ms/step - loss: 7.8487 - mae: 7.8487 - val_loss: 7.0306 - val_mae: 7.0306\n",
      "Epoch 13/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 293ms/step - loss: 7.3152 - mae: 7.3152 - val_loss: 6.9376 - val_mae: 6.9376\n",
      "Epoch 14/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 279ms/step - loss: 7.2894 - mae: 7.2894 - val_loss: 7.0760 - val_mae: 7.0760\n",
      "Epoch 15/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 289ms/step - loss: 7.6026 - mae: 7.6026 - val_loss: 7.0313 - val_mae: 7.0313\n",
      "Epoch 16/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 287ms/step - loss: 7.3178 - mae: 7.3178 - val_loss: 6.7987 - val_mae: 6.7987\n",
      "Epoch 17/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 289ms/step - loss: 7.0943 - mae: 7.0943 - val_loss: 6.7640 - val_mae: 6.7640\n",
      "Epoch 18/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 307ms/step - loss: 7.4718 - mae: 7.4718 - val_loss: 7.0570 - val_mae: 7.0570\n",
      "Epoch 19/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 310ms/step - loss: 7.4019 - mae: 7.4019 - val_loss: 7.0610 - val_mae: 7.0610\n",
      "Epoch 20/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 308ms/step - loss: 6.7315 - mae: 6.7315 - val_loss: 6.9763 - val_mae: 6.9763\n",
      "Epoch 21/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 314ms/step - loss: 6.0155 - mae: 6.0155 - val_loss: 6.9063 - val_mae: 6.9063\n",
      "Epoch 22/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 324ms/step - loss: 6.9620 - mae: 6.9620 - val_loss: 7.0219 - val_mae: 7.0219\n",
      "Epoch 23/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 337ms/step - loss: 6.3998 - mae: 6.3998 - val_loss: 6.7941 - val_mae: 6.7941\n",
      "Epoch 24/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 310ms/step - loss: 6.0035 - mae: 6.0035 - val_loss: 6.7887 - val_mae: 6.7887\n",
      "Epoch 25/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 313ms/step - loss: 7.1547 - mae: 7.1547 - val_loss: 6.8318 - val_mae: 6.8318\n",
      "Epoch 26/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 333ms/step - loss: 6.2460 - mae: 6.2460 - val_loss: 6.8151 - val_mae: 6.8151\n",
      "Epoch 27/100\n",
      "\u001B[1m8/8\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 339ms/step - loss: 6.3437 - mae: 6.3437 - val_loss: 7.2266 - val_mae: 7.2266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle sauvegardé sous 'age_estimation_model.h5'\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:46:48.860660Z",
     "start_time": "2025-01-22T16:46:47.436709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Charger le modèle sauvegardé\n",
    "model = load_model(\"age_estimation_model_brad.h5\")\n",
    "\n",
    "# Fonction pour tester avec une image\n",
    "def predict_age(image_path):\n",
    "    # Charger l'image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Erreur : Impossible de charger l'image {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Prétraiter l'image (redimensionner et normaliser)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = img / 255.0  # Normalisation\n",
    "    img = np.expand_dims(img, axis=0)  # Ajouter une dimension pour le batch\n",
    "    \n",
    "    # Effectuer la prédiction\n",
    "    predicted_age = model.predict(img)[0][0]  # Prédiction (régression)\n",
    "    \n",
    "    # Afficher le résultat\n",
    "    print(f\"Âge estimé : {predicted_age:.2f} ans\")\n",
    "\n",
    "# Tester avec une image\n",
    "predict_age(r\"C:\\Users\\33658\\Downloads\\1811_face.jpg\")\n"
   ],
   "id": "d41db8df60e843b7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000243EEC88E00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000243EEC88E00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 786ms/step\n",
      "Âge estimé : 41.22 ans\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
