{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-23T07:49:57.248643Z",
     "start_time": "2025-01-23T07:44:59.035398Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.src.utils.module_utils import tensorflow\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Charger les données depuis un fichier CSV\n",
    "data = pd.read_csv(r\"C:\\Users\\33658\\PycharmProjects\\Act-O-Matic\\Datasets\\IMDB Scrap Cropped\\Tom Cruise Crop\\index_cleaned.csv\", delimiter=\";\")\n",
    "\n",
    "# Charger et prétraiter les images\n",
    "def preprocess_data(data):\n",
    "    images, ages = [], []\n",
    "    base_path = r\"C:\\Users\\33658\\PycharmProjects\\Act-O-Matic\\Datasets\\IMDB Scrap Cropped\\Tom Cruise Crop\"\n",
    "    for i, row in data.iterrows():\n",
    "        img_name = str(row[\"Image\"]) + \"_face\"+ \".jpg\" \n",
    "        img_path = os.path.join(base_path, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"Image non trouvée : {img_path}\")\n",
    "            continue\n",
    "        img = cv2.resize(img, (224, 224))   # Redimensionner\n",
    "        img = img / 255.0                   # Normaliser\n",
    "        images.append(img)\n",
    "        ages.append(float(row[\"Age\"]))\n",
    "    return np.array(images), np.array(ages)\n",
    "\n",
    "# Prétraiter les données\n",
    "images, ages = preprocess_data(data)\n",
    "\n",
    "# Diviser en ensembles d'entraînement et de validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(images, ages, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convertir en tenseurs\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32).shuffle(1000)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)\n",
    "\n",
    "# Charger le modèle pré-entraîné MobileNetV2\n",
    "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Ajouter des couches personnalisées\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(1, activation='linear')(x)  # Activation linéaire pour une régression\n",
    "\n",
    "# Construire le modèle\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Geler les couches du modèle pré-entraîné\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=\"mean_absolute_error\",  # Erreur absolue moyenne pour la régression\n",
    "    metrics=[\"mae\"]  # Suivi de la MAE\n",
    ")\n",
    "\n",
    "# Ajouter un callback pour l'early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=100,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "model.save(\"age_estimation_model_Tom.h5\")\n",
    "model.save(\"../Models/age_estimation_model_tom.keras\")\n",
    "print(\"Modèle sauvegardé sous 'age_estimation_model_pitt.h5'\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 797ms/step - loss: 37.2444 - mae: 37.2444 - val_loss: 22.2720 - val_mae: 22.2720\n",
      "Epoch 2/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 1s/step - loss: 21.1860 - mae: 21.1860 - val_loss: 11.5513 - val_mae: 11.5513\n",
      "Epoch 3/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 690ms/step - loss: 10.6173 - mae: 10.6173 - val_loss: 13.9294 - val_mae: 13.9294\n",
      "Epoch 4/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 618ms/step - loss: 12.8102 - mae: 12.8102 - val_loss: 13.6898 - val_mae: 13.6898\n",
      "Epoch 5/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 1s/step - loss: 11.6139 - mae: 11.6139 - val_loss: 11.0756 - val_mae: 11.0756\n",
      "Epoch 6/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 706ms/step - loss: 10.7951 - mae: 10.7951 - val_loss: 10.8040 - val_mae: 10.8040\n",
      "Epoch 7/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 875ms/step - loss: 10.1540 - mae: 10.1540 - val_loss: 10.6216 - val_mae: 10.6216\n",
      "Epoch 8/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 896ms/step - loss: 10.3393 - mae: 10.3393 - val_loss: 10.8321 - val_mae: 10.8321\n",
      "Epoch 9/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 495ms/step - loss: 10.4668 - mae: 10.4668 - val_loss: 10.9824 - val_mae: 10.9824\n",
      "Epoch 10/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 414ms/step - loss: 10.3137 - mae: 10.3137 - val_loss: 10.5081 - val_mae: 10.5081\n",
      "Epoch 11/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 417ms/step - loss: 9.4134 - mae: 9.4134 - val_loss: 10.2228 - val_mae: 10.2228\n",
      "Epoch 12/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 420ms/step - loss: 9.4367 - mae: 9.4367 - val_loss: 10.0296 - val_mae: 10.0296\n",
      "Epoch 13/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 509ms/step - loss: 8.7776 - mae: 8.7776 - val_loss: 9.9166 - val_mae: 9.9166\n",
      "Epoch 14/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 453ms/step - loss: 8.4069 - mae: 8.4069 - val_loss: 9.9647 - val_mae: 9.9647\n",
      "Epoch 15/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 1s/step - loss: 8.4554 - mae: 8.4554 - val_loss: 9.8934 - val_mae: 9.8934\n",
      "Epoch 16/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 1s/step - loss: 8.8131 - mae: 8.8131 - val_loss: 9.5543 - val_mae: 9.5543\n",
      "Epoch 17/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 1s/step - loss: 9.3329 - mae: 9.3329 - val_loss: 9.6445 - val_mae: 9.6445\n",
      "Epoch 18/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 1s/step - loss: 7.6539 - mae: 7.6539 - val_loss: 10.0341 - val_mae: 10.0341\n",
      "Epoch 19/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 818ms/step - loss: 8.3467 - mae: 8.3467 - val_loss: 9.3934 - val_mae: 9.3934\n",
      "Epoch 20/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 511ms/step - loss: 8.8164 - mae: 8.8164 - val_loss: 9.3604 - val_mae: 9.3604\n",
      "Epoch 21/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 692ms/step - loss: 7.5536 - mae: 7.5536 - val_loss: 9.7621 - val_mae: 9.7621\n",
      "Epoch 22/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 1s/step - loss: 8.7889 - mae: 8.7889 - val_loss: 9.9082 - val_mae: 9.9082\n",
      "Epoch 23/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 991ms/step - loss: 7.5003 - mae: 7.5003 - val_loss: 9.9441 - val_mae: 9.9441\n",
      "Epoch 24/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 912ms/step - loss: 7.1328 - mae: 7.1328 - val_loss: 9.3676 - val_mae: 9.3676\n",
      "Epoch 25/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 878ms/step - loss: 7.6338 - mae: 7.6338 - val_loss: 9.2013 - val_mae: 9.2013\n",
      "Epoch 26/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 687ms/step - loss: 8.6295 - mae: 8.6295 - val_loss: 9.2266 - val_mae: 9.2266\n",
      "Epoch 27/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 598ms/step - loss: 8.2392 - mae: 8.2392 - val_loss: 9.3442 - val_mae: 9.3442\n",
      "Epoch 28/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 472ms/step - loss: 7.1855 - mae: 7.1855 - val_loss: 9.5905 - val_mae: 9.5905\n",
      "Epoch 29/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 436ms/step - loss: 8.9634 - mae: 8.9634 - val_loss: 9.2247 - val_mae: 9.2247\n",
      "Epoch 30/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 481ms/step - loss: 7.4165 - mae: 7.4165 - val_loss: 9.1595 - val_mae: 9.1595\n",
      "Epoch 31/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 427ms/step - loss: 8.0861 - mae: 8.0861 - val_loss: 9.5314 - val_mae: 9.5314\n",
      "Epoch 32/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 1s/step - loss: 7.2407 - mae: 7.2407 - val_loss: 9.2639 - val_mae: 9.2639\n",
      "Epoch 33/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 2s/step - loss: 7.3801 - mae: 7.3801 - val_loss: 9.1940 - val_mae: 9.1940\n",
      "Epoch 34/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 1s/step - loss: 8.0012 - mae: 8.0012 - val_loss: 9.1437 - val_mae: 9.1437\n",
      "Epoch 35/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 915ms/step - loss: 6.6616 - mae: 6.6616 - val_loss: 9.1695 - val_mae: 9.1695\n",
      "Epoch 36/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 579ms/step - loss: 7.9767 - mae: 7.9767 - val_loss: 9.2751 - val_mae: 9.2751\n",
      "Epoch 37/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 518ms/step - loss: 7.8118 - mae: 7.8118 - val_loss: 9.1693 - val_mae: 9.1693\n",
      "Epoch 38/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 484ms/step - loss: 7.4997 - mae: 7.4997 - val_loss: 8.9942 - val_mae: 8.9942\n",
      "Epoch 39/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 1s/step - loss: 7.2405 - mae: 7.2405 - val_loss: 9.1404 - val_mae: 9.1404\n",
      "Epoch 40/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 2s/step - loss: 7.9429 - mae: 7.9429 - val_loss: 9.0735 - val_mae: 9.0735\n",
      "Epoch 41/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 1s/step - loss: 6.8618 - mae: 6.8618 - val_loss: 8.8961 - val_mae: 8.8961\n",
      "Epoch 42/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 2s/step - loss: 7.9382 - mae: 7.9382 - val_loss: 9.0178 - val_mae: 9.0178\n",
      "Epoch 43/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 2s/step - loss: 7.1383 - mae: 7.1383 - val_loss: 9.0445 - val_mae: 9.0445\n",
      "Epoch 44/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 1s/step - loss: 6.6643 - mae: 6.6643 - val_loss: 8.9489 - val_mae: 8.9489\n",
      "Epoch 45/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 2s/step - loss: 7.3943 - mae: 7.3943 - val_loss: 9.0575 - val_mae: 9.0575\n",
      "Epoch 46/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 1s/step - loss: 7.6971 - mae: 7.6971 - val_loss: 9.6178 - val_mae: 9.6178\n",
      "Epoch 47/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 1s/step - loss: 6.3790 - mae: 6.3790 - val_loss: 9.2526 - val_mae: 9.2526\n",
      "Epoch 48/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 2s/step - loss: 6.8238 - mae: 6.8238 - val_loss: 9.2334 - val_mae: 9.2334\n",
      "Epoch 49/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 1s/step - loss: 6.5082 - mae: 6.5082 - val_loss: 9.6128 - val_mae: 9.6128\n",
      "Epoch 50/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 1s/step - loss: 6.8434 - mae: 6.8434 - val_loss: 9.1772 - val_mae: 9.1772\n",
      "Epoch 51/100\n",
      "\u001B[1m6/6\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 1s/step - loss: 6.2147 - mae: 6.2147 - val_loss: 8.9754 - val_mae: 8.9754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle sauvegardé sous 'age_estimation_model_brad.h5'\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T07:55:12.652638Z",
     "start_time": "2025-01-23T07:55:12.638004Z"
    }
   },
   "cell_type": "code",
   "source": "tf.keras.saving.save_model(model, 'age_estimation_model_tom.keras')",
   "id": "a7d465e43f26d605",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras' has no attribute 'saving'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaving\u001B[49m\u001B[38;5;241m.\u001B[39msave_model(model, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mage_estimation_model_Tom.keras\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'tensorflow.keras' has no attribute 'saving'"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T07:55:40.690011Z",
     "start_time": "2025-01-23T07:55:39.084005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Charger le modèle sauvegardé\n",
    "model = load_model(\"age_estimation_model_Tom.h5\")\n",
    "\n",
    "# Fonction pour tester avec une image\n",
    "def predict_age(image_path):\n",
    "    # Charger l'image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Erreur : Impossible de charger l'image {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Prétraiter l'image (redimensionner et normaliser)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = img / 255.0  # Normalisation\n",
    "    img = np.expand_dims(img, axis=0)  # Ajouter une dimension pour le batch\n",
    "    \n",
    "    # Effectuer la prédiction\n",
    "    predicted_age = model.predict(img)[0][0]  # Prédiction (régression)\n",
    "    \n",
    "    # Afficher le résultat\n",
    "    print(f\"Âge estimé : {predicted_age:.2f} ans\")\n",
    "\n",
    "# Tester avec une image\n",
    "predict_age(r\"C:\\Users\\33658\\Downloads\\braff1.jpg\")\n"
   ],
   "id": "d41db8df60e843b7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 1s/step\n",
      "Âge estimé : 30.66 ans\n"
     ]
    }
   ],
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
